Configuration File and gdeploy Features
=======================================

This section explains the gdeploy configuration file format, and the available
features. There is no rule to name the configuration files in gdeploy as long as
the file name is mentioned with *-c* option for gdeploy. For example
gluster.conf, gluster, gluster.txt, gluster.config are all valid names for a
gdeploy configuration file.

gdeploy configuration file is split into two parts.

1. Inventory
2. Features

**1. Inventory**

The section is named [hosts], this is a mandatory section, hosts that are to be
configured have to be listed in this section.

Hostnames or ip addresses have to be listed one per line. For example::

  [hosts]
  10.0.0.1
  10.0.0.2
  10.0.0.3

.. _rst_features:

**2. Features**

gdeploy supports a lot of features, a comprehensive list can be found in the
sample file `here
<https://github.com/gluster/gdeploy/blob/master/examples/gluster.conf.sample>`_.

And more examples can be found `here
<https://github.com/gluster/gdeploy/tree/master/examples>`_.

gdeploy was initially written to create and configure GlusterFS volumes, now it
supports more features than just creating and configuring GlusterFS
volumes. The following are the features supported:

1. `LVM`_

   a. `[backend-setup]`_ section
   b. `PV`_
   c. `VG`_
   d. `LV`_

2. `Subscription Manager`_

   a. `Register`_
   b. `Unregister`_
   c. `Enable`_
   d. `Disable`_
   e. `Attach pools`_

3. `yum`_

   a. `Install packages`_
   b. `Uninstall packages`_

4. `systemd`_

   a. `Enable Service`_
   b. `Disable Service`_
   c. `Start Service`_
   d. `Stop Service`_
   e. `Restart Service`_

5. `shell`_

6. `script`_

7. `firewalld`_

8. `Updating a file`_

9. `Creating GlusterFS Volumes`_

LVM
^^^

gdeploy allows to setup backend for GlusterFS in two ways. User can either setup
backend using the modules: *pv*, *vg*, and *lv* or user can use the
*backend-setup* module to automatically create the bricks, gdeploy creates
thinpools internally.

The *backend-setup* module setups up a thin-pool by default and applies default
performance recommendations. However, if the user has a different use case which
demands more than one LV, and a combination of thin and thick pools then
*backend-setup* is of no help. The user can use PV, VG, and LV modules to
achieve this.

[backend-setup]
---------------

*backend-setup* module allows the user to create bricks for GlusterFS nodes. If
the user needs more control on creating PV, VG, and LV refer the documentation
on respective sections.

In the below example, the module creates a thinpool LV using devices sdb, sdc on
all the nodes listed in the hosts section. Refer example `2x2-volume-create.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/2x2-volume-create.conf>`_
for more complete example.::

  [backend-setup]
  devices=sdb,sdc
  vgs=vg1,vg2
  pools=pool1,pool2
  lvs=lv1,lv2
  mountpoints=/mnt/data1,/mnt/data2
  brick_dirs=/mnt/data1/1,/mnt/data2/2

If *backend-setup* has to be done for particular hosts in the *inventory* then
the section would be written like::

  [backend-setup:10.0.0.100]    # Backend would be setup on just 10.0.0.100
  devices=sdb,sdc
  vgs=vg1,vg2
  pools=pool1,pool2
  lvs=lv1,lv2
  mountpoints=/mnt/data1,/mnt/data2
  brick_dirs=/mnt/data1/1,/mnt/data2/2

  [backend-setup:10.0.0.{1..10}]    # Backend would be setup on hosts 10.0.0.1
                                      through 10.0.0.10
  devices=sdb,sdc
  vgs=vg1,vg2
  pools=pool1,pool2
  lvs=lv1,lv2
  mountpoints=/mnt/data1,/mnt/data2
  brick_dirs=/mnt/data1/1,/mnt/data2/2


*backend-setup* section supports the following variables:

1. devices - List of comma separated devices to use.
2. vgs - Names of the vgs. The number of vg names should match the number of
   devices. If name is not mentioned, default names will be generated by gdeploy.
3. pools - Name of the thinpools. If name is not mentioned, default names will
   be generated by gdeploy.
4. lvs - Logical volume names. If name is not mentioned, default names will be
   generated by gdeploy.
5. size - Size of the logical volume
6. mountpoints - Mountpoint directories. Where the logical volumes have to be
   mounted.
7. brick_dirs - The brick directories to use for creating the volume.
8. ssd - This variable is set if caching has to be added.

PV
---

The [pv] section allows user to create physical volumes on the given disks.

Example 1: Create a few physical volumes::

  [pv]
  action=create
  devices=vdb,vdc,vdd

Example 2: Create a few physical volumes on a certain host::

  [pv:10.0.5.2]
  action=create
  devices=vdb,vdc,vdd

Example 3: Expand an already created pv::

  [pv]
  action=resize
  devices=vdb
  expand=yes

Example 4: Shrink an already created pv::

  [pv]
  action=resize
  devices=vdb
  shrink=100G

VG
---
This module is used to create and extend volume groups. The vg module supports
the following variables.

1. action - Action can be one of create or extend.
2. pvname - PVs to use to create the volume. For more than one PV use comma separated values.
3. vgname - The name of the vg. If no name is provided GLUSTER_vg will be used as default name.
4. one-to-one - If set to yes, one-to-one mapping will be done between pv and vg.

If action is set to extend, the vg will be extended to include pv provided.

Refer `hc.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/hc.conf>`_ for
complete example.

Example1: Create a vg named images_vg with two PVs::

  [vg]
  action=create
  vgname=images_vg
  pvname=sdb,sdc

Example2: Create two vgs named rhgs_vg1 and rhgs_vg2 with two PVs::

  [vg]
  action=create
  vgname=rhgs_vg
  pvname=sdb,sdc
  one-to-one=yes

Example3: Extend an existing vg with the given disk::

  [vg]
  action=extend
  vgname=rhgs_images
  pvname=sdc

LV
---
This module is used to create, setup-cache, and convert logical volumes. The lv
module supports the following variables:

  1. action - The action variable allows three values *create*, *setup-cache*,
     *convert*, and *change*.

If the action is create, the following options are supported:

  1. lvname - The name of the logical volume, this is an optional field. Default
     is GLUSTER_lv
  2. poolname - Name of the thinpool volume name, this is an optional
     field. Default is GLUSTER_pool
  3. lvtype - Type of the logical volume to be created, allowed values are
     *thin* and *thick*. This is an optional field, default is thick.
  4. size - Size of the logical volume volume. Default is to take all available
     space on the vg.
  5. extent - Extent size, default is 100%FREE
  6. force - Force lv create, do not ask any questions. Allowed values *yes*,
     *no*. This is an optional field, default is yes.
  7. vgname - Name of the volume group to use.
  8. pvname - Name of the physical volume to use.
  9. chunksize - Size of chunk for snapshot.
  10. poolmetadatasize - Sets the size of pool's metadata logical volume.
  11. virtualsize - Creates a thinly provisioned device or a sparse device of
      the given size.
  12. mkfs - Creates a filesystem of the given type. Default is to use xfs.
  13. mkfs-opts - mkfs options.
  14. mount - Mount the logical volume.

If the action is setup-cache, the below options are supported:

  1. ssd - Name of the ssd device. For example sda/vda/ … to setup cache.
  2. vgname - Name of the volume group.
  3. poolname - Name of the pool.
  4. cache_meta_lv - Due to requirements from dm-cache (the kernel driver), LVM
     further splits the cache pool LV into two devices - the cache data LV and
     cache metadata LV. Provide the cache_meta_lv name here.
  5. cache_meta_lvsize - Size of the cache meta lv.
  6. cache_lv - Name of the cache data lv.
  7. cache_lvsize - Size of the cache data.
  8. force - Force

If the action is convert, the below options are supported:

  1. lvtype - type of the lv, available options are thin and thick
  2. force - Force the lvconvert, default is yes.
  3. vgname - Name of the volume group.
  4. poolmetadata - Specifies  cache  or thin pool metadata logical volume.
  5. cachemode - Allowed values writeback, writethrough. Default is writethrough.
  6. cachepool - This  argument  is  necessary when converting a logical volume
     to a cache LV. Name of the cachepool.
  7. lvname - Name of the logical volume.
  8. chunksize - Gives the size of chunk for snapshot, cache pool and thin pool
     logical volumes. Default unit is in kilobytes.
  9. poolmetadataspare - Controls creation and maintanence of pool metadata
     spare logical volume that will be used for automated  pool  recovery.
  10. thinpool - Specifies or converts logical volume into a thin pool's data
      volume. Volume’s name or path has to be given.

If the action is change, the below options are supported:

  1. lvname - Name of the logical volume.
  2. vgname - Name of the volume group.
  3. zero - Set  zeroing mode for thin pool.


Example 1: Create a thin LV::

  [lv]
  action=create
  vgname=RHGS_vg1
  poolname=lvthinpool
  lvtype=thinpool
  poolmetadatasize=10MB
  chunksize=1024k
  size=30GB

Example 2: Create a thick LV::

  [lv]
  action=create
  vgname=RHGS_vg1
  lvname=engine_lv
  lvtype=thick
  size=10GB
  mount=/rhgs/brick1


If there are more than one LV, the LVs can be created by numbering the LV
sections, like [lv1], [lv2] ...

Refer `hc.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/hc.conf>`_ for
complete example.

Subscription Manager
^^^^^^^^^^^^^^^^^^^^

This module is used to subscribe/unsubscribe to channels, attach a pool, enable
repos etc. Subscription Manager module is named RH-subscription
The RH-subscription module allows the following variables:

  1. action - This variable allows the following values, *register*,
     *attach-pool*, *enable-repos*, *disable-repos*, *unregister*.


Register
--------
If the action is *register* the following options are supported:

1. username/activationkey - Username or activationkey
2. password/actiavtionkey - Password or activation key
3. auto-attach - true / false
4. pool - Name of the pool
5. repos - Repos to subscribe to
6. disable-repos - Repo names to disable. Leaving black will disable all the
   repos

For example::

  [RH-subscription1]
  action=register
  username=user@user.com
  password=<passwd>
  pool=<pool>


Unregister
----------
If the action is *unregister* the systems will be unregistered.


Enable
------
If the action is *enable-repos* the following options are supported:

1. repos - List of comma separated repos that are to be subscribed to.

Disable
-------
If the action is *disable-repos* the following options are supported:

1. repos - List of comma separated repos that are to be subscribed to.

Attach pools
------------
If the action is *attach-pool* the following options are supported:

1. pool - Pool name to be attached.

Refer `hc.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/hc.conf>`_ for
complete example.

yum
^^^

This module is used to install or remove rpm packages, with the yum module we
can add repos during the install operation.

**If a single configuration has more than one yum section, then the sections
have to be numbered like [yum-1], [yum-2], [yum-3] ...**

1. *action* - This variable allows two values *install* and *remove*.

Install packages
----------------

If the action is install the following options are supported:

1. *packages* - Comma separated list of packages that are to be installed.
2. *repos* - The repositories that have to be added.
3. *gpgcheck* - yes/no values have to be provided.
4. *update* - yes/no; Whether yum update has to be initiated.

For example::

  [yum]
  action=install
  gpgcheck=no
  # Repos should be an url; eg: http://repo-pointing-glusterfs-builds
  repos=<glusterfs.repo>,<vdsm.repo>
  packages=vdsm,vdsm-gluster,ovirt-hosted-engine-setup,screen,gluster-nagios-addons,xauth
  update=yes

Install a package on a particular host::

  [yum:host1]
  action=install
  gpgcheck=no
  packages=rhevm-appliance

Uninstall packages
------------------

If the action is *remove* then only one option has to be provided:

1. *remove* - The comma separated list of packages to be removed.

Unstall a package on a particular host::

  [yum:host1]
  action=remove
  packages=rhevm-appliance

systemd
^^^^^^^

[service] module in gdeploy adds systemd support. The *service* module allows
user to *start*, *stop*, *restart*, *reload*, *enable*, or *disable* a
service. The action variable specifies these values.

Enable Service
--------------

When the *action* variable is set to *enable* the *service* variable has to be
set. For example::

  [service]
  action=enable
  service=ntpd


Disable Service
---------------

When the *action* variable is set to *enable* the *service* variable has to be
set. For example::

  [service]
  action=enable
  service=ntpd


Start Service
-------------

When the *action* variable is set to *start* the *service* variable has to be
set. For example, below configuration starts the ntpd service ::

  [service]
  action=start
  service=ntpd


Stop Service
------------

When the *action* variable is set to *stop* the *service* variable has to be
set. For example::

  [service]
  action=stop
  service=ntpd


Restart Service
---------------

When the *action* variable is set to *restart* the *service* variable has to be
set. For example::

  [service]
  action=restart
  service=ntpd

shell
^^^^^

shell module allows user to run shell commands on the remote nodes.

Currently shell provides a single *action* variable with value *execute*. And a
*command* variable with any valid shell command as value.

The below config will execute vdsm-tool on all the nodes::

  [shell]
  action=execute
  command=vdsm-tool configure --force

Refer `hc.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/hc.conf>`_ for
complete example.

script
^^^^^^

script module enables user to execute a script/binary on the remote
machine. action variable is set to execute. Allows user to specify two variables
*file* and *args*.

1. file - An executable on the local machine.
2. args - Arguments to the above program.

Example: Execute script disable-multipath.sh on all the remote nodes listed in *hosts* section::

  [script]
  action=execute
  file=/usr/share/ansible/gdeploy/scripts/disable-multipath.sh

Refer `hc.conf
<https://github.com/gluster-deploy/gdeploy/blob/2.0/examples/hc.conf>`_ for a
complete example.


firewalld
^^^^^^^^^

firewalld module allows the user to manipulate firewall rules. *action* variable
supports two values *add* and *delete*.
Both *add* and *delete* support the following variables:

1. ports/services - The ports or services to add to firewall.
2. permanent - Whether to make the entry permanent. Allowed values are true/false
3. zone - Default zone is public

For example::

  [firewalld]
  action=add
  ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp
  services=glusterfs

Updating a file
^^^^^^^^^^^^^^^

*update-file* module allows user to copy a file, edit a line in a file, or add
new lines to a file. action variable can be any of *copy*, *edit*, or *add*.

When the *action* variable is set to *copy*, the following variables are
supported.

1. src - The source path of the file to be copied from.
2. dest - The destination path on the remote machine to where the file is to be
   copied to.

When the *action* variable is set to *edit*, the following variables are
supported.

1. dest - The destination file name which has to be edited.
2. replace - A regular expression, which will match a line that will be replaced.
3. line - Text that has to be replaced.

When the *action* variable is set to *add*, the following variables are
supported.

1. dest - File on the remote machine to which a line has to be added.
2. line - Line which has to be added to the file. Line will be added towards the end of the file.

Example 1: Copy a file to a remote machine ::

  [update-file]
  action=copy
  src=/tmp/foo.cfg
  dest=/etc/nagios/nrpe.cfg


Example 2: Edit a line in the remote machine, in the below example lines that
have allowed_hosts will be replaced with allowed_hosts=host.redhat.com ::

  [update-file]
  action=edit
  dest=/etc/nagios/nrpe.cfg
  replace=allowed_hosts
  line=allowed_hosts=host.redhat.com

Example 3: Add a line to the end of a file ::

  [update-file]
  action=add
  dest=/etc/ntp.conf
  line=server clock.redhat.com iburst

Creating GlusterFS Volumes
^^^^^^^^^^^^^^^^^^^^^^^^^^

The *volume* module allows users to create volume using a specified list of
hosts and bricks. Volume section supports the following variables:

1. volname - Name of the volume, if no name is provided gdeploy generates a
   volume name.
2. action - Action supports the following values *create*, *delete*,
   *add-brick*, *remove-brick*, *rebalance*, and *set*.
3. brick_dirs - This variable specifies the brick directories to use. The
   brick_dirs variable can take values in ip:brick_dir format or just brick_dir
   format. For example:

   brick_dir=10.0.0.1:/mnt/data1/1,10.0.0.2:/mnt/data2/2

   Or

   brick_dir=/mnt/data1/1,/mnt/data2/2
4. transport - The transport type. Possible values are tcp,tcp,rdma,rdma
5. replica_count - The replication count for replica volumes.
6. force - If set to yes, force is used while creating volumes.
7. disperse - Identifies if the volume should be disperse. Possible options are
   [yes, no].
8. disperse_count - Optional argument. If none given, the number of bricks
   specified in the commandline is taken as the disperse_count value.
9. redundancy_count - If redundancy_count is not specified, and if *disperse* is
   yes, it's default value is computed so that it generates an optimal
   configuration.

Example 1::

  [volume]
  action=create
  volname=foo
  transport=tcp
  replica_count=2
  force=yes

Example 2::

  [backend-setup]
  devices=sdb,sdc
  vgs=vg1,vg2
  pools=pool1,pool2
  lvs=lv1,lv2
  brick_dirs=/gluster/brick/brick{1,2}

  # If backend-setup is different for each host
  # [backend-setup:10.70.46.13]
  # devices=sdb
  # brick_dirs=/gluster/brick/brick1
  #
  # [backend-setup:10.70.46.17]
  # devices=sda,sdb,sdc
  # brick_dirs=/gluster/brick/brick{1,2,3}
  #

  [volume]
  action=create
  volname=sample_volname
  replica=yes
  replica_count=2
  force=yes
